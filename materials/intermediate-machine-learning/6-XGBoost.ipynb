{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "The most accurate modeling technique for structured data.\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) là một thuật toán học máy phổ biến được sử dụng để giải quyết các vấn đề dự đoán và phân loại. Nó là một biến thể của thuật toán Gradient Boosting và đã đạt được sự phổ biến lớn trong cộng đồng khoa học dữ liệu và học máy.\n",
    "\n",
    "XGBoost hoạt động bằng cách xây dựng một loạt cây quyết định (Decision Trees) theo cách tuần tự, trong đó mỗi cây cố gắng cải thiện kết quả so với cây trước đó. Quy trình này được thực hiện theo các bước sau:\n",
    "\n",
    "- Khởi tạo cây đầu tiên: XGBoost bắt đầu bằng việc tạo một cây quyết định đơn giản, thường là một cây có một nút gốc duy nhất. Cây này sẽ được cải thiện thông qua các bước tiếp theo.\n",
    "\n",
    "- Tối ưu hóa hàm mất mát: Mục tiêu là tối ưu hóa một hàm mất mát (loss function) để đánh giá sự sai lệch giữa dự đoán và giá trị thực tế. XGBoost sẽ cố gắng điều chỉnh cây để làm giảm hàm mất mát này thông qua việc tối ưu hóa các tham số của cây.\n",
    "\n",
    "- Thêm cây mới: Sau khi tối ưu hóa cây hiện tại, một cây quyết định mới sẽ được thêm vào, với mục tiêu là giảm thêm sai số trong dự đoán.\n",
    "\n",
    "- Các cây kết hợp: XGBoost tổng hợp kết quả từ tất cả các cây để tạo ra dự đoán cuối cùng. Điều này có thể thực hiện thông qua trung bình hoặc trọng số các cây theo hiệu suất của chúng.\n",
    "\n",
    "- Kiểm soát quá khớp: XGBoost cung cấp các cơ chế kiểm soát quá khớp (overfitting) như giới hạn độ sâu của cây, cân nhắc lề (regularization), và tỷ lệ học tập (learning rate) để đảm bảo mô hình không quá phức tạp và áp dụng được cho dữ liệu mới.\n",
    "\n",
    "XGBoost là một trong những thuật toán học máy mạnh mẽ và hiệu quả, thường được sử dụng cho các nhiệm vụ như dự đoán giá cổ phiếu, phân loại hình ảnh, hoặc bất kỳ ứng dụng nào yêu cầu dự đoán và phân loại dữ liệu.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost và LightGBM đều là các thuật toán Gradient Boosting Machine (GBM) được sử dụng rộng rãi trong lĩnh vực học máy. Dưới đây là một số điểm khác biệt giữa chúng:\n",
    "\n",
    "**Thuật toán tối ưu hóa:**\n",
    "\n",
    "- XGBoost (Extreme Gradient Boosting): Sử dụng thuật toán tối ưu hóa Gradient Descent. Nó cải thiện từng cây một bằng cách sử dụng Gradient Descent và tham số tùy chỉnh như 'learning rate' để điều chỉnh quá khớp.\n",
    "- LightGBM: Sử dụng thuật toán tối ưu hóa Gradient-based One-Side Sampling (GOSS) và Exclusive Feature Bundling (EFB). GOSS giúp tăng tốc quá trình huấn luyện bằng cách loại bỏ các ví dụ dễ học và tập trung vào ví dụ khó học. EFB giúp tối ưu hóa việc tạo ra các tài sản đặc trưng (feature assets).\n",
    "  \n",
    "**Tốc độ và hiệu năng:**\n",
    "\n",
    "- XGBoost: Tốc độ huấn luyện tương đối chậm hơn so với LightGBM. Tuy nhiên, nó vẫn nhanh hơn nhiều so với nhiều thuật toán khác.\n",
    "- LightGBM: Nổi tiếng với tốc độ nhanh hơn đáng kể. Điều này đặc biệt hữu ích khi bạn có tập dữ liệu lớn và muốn huấn luyện mô hình nhanh chóng.\n",
    "  \n",
    "**Quản lý bộ nhớ:**\n",
    "\n",
    "- XGBoost: Đòi hỏi một lượng bộ nhớ lớn để lưu trữ dữ liệu, đặc biệt là khi bạn có nhiều cây quyết định trong mô hình.\n",
    "- LightGBM: Sử dụng cách quản lý bộ nhớ hiệu quả hơn. Nó sử dụng cấu trúc dữ liệu histogram để giảm bộ nhớ cần thiết, làm cho nó thích hợp cho dữ liệu lớn hơn.\n",
    "  \n",
    "**Quá khớp (Overfitting):**\n",
    "\n",
    "- XGBoost: Để kiểm soát quá khớp, bạn cần tinh chỉnh các tham số như 'learning rate' và 'max_depth'. Quá trình này có thể tốn thời gian.\n",
    "- LightGBM: LightGBM cung cấp cơ chế kiểm soát quá khớp tự động thông qua GOSS và EFB.\n",
    "  \n",
    "**Thiết lập và tinh chỉnh tham số:**\n",
    "\n",
    "- XGBoost: Cần tinh chỉnh tham số một cách cẩn thận và thường đòi hỏi hiểu biết sâu về mô hình.\n",
    "- LightGBM: Có thể yêu cầu ít công sức hơn trong việc tinh chỉnh tham số nhờ vào việc tự động kiểm soát quá khớp và tốc độ nhanh.\n",
    "  \n",
    "Chọn giữa XGBoost và LightGBM phụ thuộc vào tình huống cụ thể của dự án của bạn. Nếu bạn cần một tốc độ nhanh và quản lý bộ nhớ hiệu quả, LightGBM có thể là lựa chọn tốt. Tuy nhiên, nếu bạn có thời gian để tinh chỉnh và muốn kiểm soát cẩn thận quá khớp, XGBoost có thể phù hợp hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
